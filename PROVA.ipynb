{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -2394.12, Successes: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 274\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 274\u001b[0m     trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 246\u001b[0m, in \u001b[0;36mtrain_ddpg\u001b[0;34m(env, num_episodes)\u001b[0m\n\u001b[1;32m    244\u001b[0m agent\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mpush(transition)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    248\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[2], line 147\u001b[0m, in \u001b[0;36mDDPGAgent.update\u001b[0;34m(self, gamma, tau, device)\u001b[0m\n\u001b[1;32m    145\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(states))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    146\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(actions))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 147\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    148\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(next_states))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    149\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(dones))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/_tensor.py:1058\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m __array_priority__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# prefer Tensor ops over numpy ones\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from environment import TrackingEnv\n",
    "import datetime\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MAX_MEMORY = 4000\n",
    "NUM_NEURONS = 256\n",
    "LR_CRITIC = 0.0001\n",
    "LR_ACTOR = 0.0005\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "K_EPOCH = 10\n",
    "ENTROPY_COEFF = 0.01\n",
    "CHECKPOINT_INTERVAL = 500\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = f\"runs/ppo_run_PROVA{now}\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "def compute_advantages(rewards, values, dones, gamma=GAMMA, lam=LAMBDA):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = torch.cat([values, torch.tensor([0], dtype=values.dtype, device=values.device)])  # Append a 0 for the last state\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    return torch.tensor(advantages, dtype=torch.float)\n",
    "\n",
    "def compute_returns(rewards, dones, gamma=0.99):\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "        G = reward + gamma * G * (1 - done)  # Reset if episode ended\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        self.noise_std = 0.01\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        \n",
    "        # Output per la media (mu)\n",
    "        self.mu_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "        \n",
    "        # Output per la deviazione standard (log_sigma)\n",
    "        self.log_sigma_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "\n",
    "    def add_noise_to_target(self, state):\n",
    "        state = state.clone()\n",
    "        if state.dim() == 1:\n",
    "            state[2:4] += torch.normal(0.0, self.noise_std, size=(2,), device=state.device)\n",
    "        else:\n",
    "            state[:, 2:4] += torch.normal(0.0, self.noise_std, size=state[:, 2:4].shape, device=state.device)\n",
    "        return state\n",
    "\n",
    "\n",
    "    def forward(self, state, training=True):\n",
    "        \n",
    "        if training:\n",
    "            state = self.add_noise_to_target(state)\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Media delle azioni\n",
    "        mu = self.mu_layer(x)\n",
    "        mu = 5.0*torch.tanh(mu)    # azioni in [-5, 5]\n",
    "        \n",
    "        # Deviazione standard (softplus per garantire positività)\n",
    "        log_sigma = self.log_sigma_layer(x)\n",
    "        sigma = F.softplus(log_sigma) + 1e-5 # 1e-5 per evitare log(0)\n",
    "        #sigma = F.softplus(log_sigma) + exploration_term + 1e-5 # 1e-5 per evitare log(0)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc3 = nn.Linear(NUM_NEURONS, 1)\n",
    "        self.noise_std = 0.01\n",
    "\n",
    "    def add_noise_to_target(self, state):\n",
    "        state = state.clone()\n",
    "        if state.dim() == 1:\n",
    "            state[2:4] += torch.normal(0.0, self.noise_std, size=(2,), device=state.device)\n",
    "        else:\n",
    "            state[:, 2:4] += torch.normal(0.0, self.noise_std, size=state[:, 2:4].shape, device=state.device)\n",
    "        return state\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            x = self.add_noise_to_target(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.actor = PolicyNet(num_inputs, num_actions)\n",
    "        self.critic = ValueNet(num_inputs)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "        self.buffer = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        mu, sigma = self.actor.forward(state, training)\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "        action = dist.rsample()  # Usa reparametrization trick per il backprop\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)  # Somma log-prob per dimensione azione\n",
    "        return action.detach(), log_prob\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def reward_function(self, state, action, next_state, step, tolerance, rimbalzato, attached_counter):\n",
    "        #reward = - torch.norm(next_state[:2] - state[2:4]) -1\n",
    "        pos = state[:2]\n",
    "        target = state[2:4]              # target(t)\n",
    "        next_pos = next_state[:2]        # agent(t+1)\n",
    "\n",
    "        to_target = F.normalize(target - pos, dim=0)\n",
    "        action_dir = F.normalize(action, dim=0)\n",
    "        direction_reward = torch.dot(action_dir, to_target)\n",
    "        direction_penalty = 1.0 - direction_reward\n",
    "\n",
    "        dist_before = torch.norm(pos - target)\n",
    "        dist_after = torch.norm(next_pos - target)  # sempre verso target(t)\n",
    "        progress = dist_before - dist_after\n",
    "\n",
    "        reward = - 5 * direction_penalty #+ progress\n",
    "\n",
    "        if torch.norm(next_state[:2] - state[2:4]) < tolerance:\n",
    "            reward += 100 #50\n",
    "        \n",
    "        if rimbalzato:\n",
    "            reward -= 5\n",
    "\n",
    "        return reward - 1\n",
    "        \n",
    "    \n",
    "    def update(self):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        states, actions, rewards, dones, old_log_probs = zip(*self.buffer)\n",
    "        old_log_probs = torch.stack(old_log_probs)\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions).float()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        values = self.critic(states, training=True).squeeze()\n",
    "        advantages = compute_advantages(rewards, values.detach(), dones)\n",
    "        returns = compute_returns(rewards, dones)  # Compute Monte Carlo returns\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        \n",
    "        for _ in range(K_EPOCH):\n",
    "            means, stds = self.actor(states, training=True)\n",
    "            dist = torch.distributions.Normal(means, stds)\n",
    "            entropy = dist.entropy().sum(dim=-1).mean()  # media sull’intero batch\n",
    "            new_log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1-CLIP_EPS, 1+CLIP_EPS) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEFF * entropy\n",
    "            values_pred =self.critic(states, training=True).squeeze()\n",
    "            value_loss = F.mse_loss(values_pred, returns)\n",
    "\n",
    "            total_loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            #policy_loss.backward()\n",
    "            #value_loss.backward()\n",
    "            total_loss.backward()\n",
    "            self.optimizer_actor.step()\n",
    "            self.optimizer_critic.step()\n",
    "        \n",
    "        self.buffer.clear()\n",
    "\n",
    "def save_checkpoint(agent, episode):\n",
    "    path = os.path.join(RUN_DIR, f\"checkpoint_ep{episode}.pth\")\n",
    "    torch.save({\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_state_dict': agent.critic.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def plot_and_save(rewards, successes):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards, label='Total Reward')\n",
    "    plt.plot(np.convolve(successes, np.ones(10)/10, mode='valid'), label='Success Rate (10)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.title('PPO Training Progress')\n",
    "    plt.savefig(os.path.join(RUN_DIR, 'training_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "def save_trajectory_plot(trajectory, target_trajectory, episode, tag=\"trajectory\"):\n",
    "    trajectory = np.array(trajectory)\n",
    "    target_trajectory = np.array(target_trajectory)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(trajectory[:, 0], trajectory[:, 1], label=\"Agente\", color='blue')\n",
    "    plt.plot(target_trajectory[:, 0], target_trajectory[:, 1], label=\"Target\", color='red')\n",
    "    plt.scatter(*trajectory[0], color='green', label='Start agente', s=100)\n",
    "    plt.scatter(*target_trajectory[0], color='yellow', label='Start target', s=100)\n",
    "    plt.scatter(*target_trajectory[-1], color='red', label='End agente', s=100)\n",
    "    plt.scatter(target_trajectory[-5:, 0], target_trajectory[-5:, 1], color='orange', label='Ultimi target', s=10)\n",
    "    plt.scatter(trajectory[-5:, 0], trajectory[-5:, 1], color='purple', label='Ultimi agente', s=10)\n",
    "    plt.title(f\"{tag.capitalize()} - Episodio {episode}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "    plt.savefig(os.path.join(RUN_DIR, f\"{tag}_ep{episode}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def train_ppo(env=None, num_episodes=10001):\n",
    "    if env is None:\n",
    "        env = TrackingEnv()\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "    agent = PPOAgent(num_inputs, num_actions)\n",
    "    reward_history = []\n",
    "    success_history = []\n",
    "    counter = 0\n",
    "    tolerance = 0.02\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        trajectory = []\n",
    "        target_trajectory = []\n",
    "        step = 0\n",
    "        attached_counter = 0\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "            trajectory.append(state[:2].detach().numpy())\n",
    "            target_trajectory.append(state[2:4].detach().numpy())\n",
    "            action, log_prob = agent.get_action(state)\n",
    "            next_state, _, done, truncated, _, rimbalzato = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            if torch.norm(next_state[:2] - next_state[2:4]) < tolerance or truncated:\n",
    "                done = True\n",
    "            reward = agent.reward_function(state, action, next_state, step, tolerance, rimbalzato, attached_counter)\n",
    "            agent.store_transition((state, action, reward, done, log_prob))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if torch.norm(next_state[:2] - next_state[2:4]) < tolerance:\n",
    "            counter += 1\n",
    "            success_history.append(1)\n",
    "            if counter % 100 == 0:\n",
    "                save_trajectory_plot(trajectory, target_trajectory, episode, tag=\"success\")\n",
    "        else:\n",
    "            success_history.append(0)\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        if episode % 500 == 0:\n",
    "            print(f\"Episode: {episode}, Reward: {total_reward:.2f}, Successes: {counter}\")\n",
    "            save_trajectory_plot(trajectory, target_trajectory, episode)\n",
    "            save_checkpoint(agent, episode)\n",
    "\n",
    "        if len(agent.buffer) >= MAX_MEMORY:\n",
    "            agent.update()\n",
    "\n",
    "    env.close()\n",
    "    np.save(os.path.join(RUN_DIR, 'rewards.npy'), reward_history)\n",
    "    np.save(os.path.join(RUN_DIR, 'successes.npy'), success_history)\n",
    "    plot_and_save(reward_history, success_history)\n",
    "    print(\"Training finished. Artifacts saved in:\", RUN_DIR)\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent = train_ppo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
