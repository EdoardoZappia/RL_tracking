{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -2394.12, Successes: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 274\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 274\u001b[0m     trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 246\u001b[0m, in \u001b[0;36mtrain_ddpg\u001b[0;34m(env, num_episodes)\u001b[0m\n\u001b[1;32m    244\u001b[0m agent\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mpush(transition)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    248\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[2], line 147\u001b[0m, in \u001b[0;36mDDPGAgent.update\u001b[0;34m(self, gamma, tau, device)\u001b[0m\n\u001b[1;32m    145\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(states))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    146\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(actions))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 147\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    148\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(next_states))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    149\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray(dones))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/_tensor.py:1058\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# Numpy array interface, to support `numpy.asarray(tensor) -> ndarray`\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m __array_priority__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# prefer Tensor ops over numpy ones\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from environment import TrackingEnv\n",
    "import random\n",
    "from collections import deque\n",
    "import datetime\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "NUM_NEURONS = 256\n",
    "LR_ACTOR = 0.001\n",
    "LR_CRITIC = 0.001\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "EARLY_STOPPING_EPISODES = 30\n",
    "CHECKPOINT_INTERVAL = 500\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = f\"runs/PROVA{now}\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc3 = nn.Linear(NUM_NEURONS, action_dim)\n",
    "        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)\n",
    "        nn.init.uniform_(self.fc3.bias, -3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, training=True):\n",
    "        #if training:\n",
    "            #noise = torch.normal(mean=0.0, std=0.01, size=state.shape)  # noise (std half tolerance)\n",
    "            #state = state + noise   # state with noise\n",
    "\n",
    "        # Per gestire batch e singoli stati\n",
    "        if training:\n",
    "            if state.dim() == 1:\n",
    "                noise = torch.normal(mean=0.0, std=0.01, size=(2,), device=state.device)\n",
    "                state = state.clone()\n",
    "                state[2:4] = state[2:4] + noise\n",
    "            else:\n",
    "                noise = torch.normal(mean=0.0, std=0.01, size=state[:, 2:4].shape, device=state.device)\n",
    "                state = state.clone()\n",
    "                state[:, 2:4] = state[:, 2:4] + noise\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action = torch.tanh(self.fc3(x)) * 5.0\n",
    "        return action\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc3 = nn.Linear(NUM_NEURONS, 1)\n",
    "\n",
    "    def forward(self, state, action, training=True):\n",
    "        #if training:\n",
    "            #noise = torch.normal(mean=0.0, std=0.01, size=state.shape)  # noise (std half tolerance)\n",
    "            #state = state + noise   # state with noise\n",
    "\n",
    "        # Per gestire batch e singoli stati\n",
    "        if training:\n",
    "            if state.dim() == 1:\n",
    "                noise = torch.normal(mean=0.0, std=0.01, size=(2,), device=state.device)\n",
    "                state = state.clone()\n",
    "                state[2:4] = state[2:4] + noise\n",
    "            else:\n",
    "                noise = torch.normal(mean=0.0, std=0.01, size=state[:, 2:4].shape, device=state.device)\n",
    "                state = state.clone()\n",
    "                state[:, 2:4] = state[:, 2:4] + noise\n",
    "\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DDPGAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DDPGAgent, self).__init__()\n",
    "        self.actor = PolicyNet(state_dim, action_dim)\n",
    "        self.actor_target = PolicyNet(state_dim, action_dim)\n",
    "        self.critic = QNet(state_dim, action_dim)\n",
    "        self.critic_target = QNet(state_dim, action_dim)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "        self.buffer = ReplayBuffer(50000)\n",
    "        self.batch_size = 128\n",
    "        self.noise_std = 0.5\n",
    "        self.min_noise_std = 0.01\n",
    "        self.noise_decay = 0.999\n",
    "\n",
    "    def reward_function(self, state, action, next_state, step, tolerance, rimbalzato, attached_counter):\n",
    "        pos = state[:2]\n",
    "        target = state[2:4]              # target(t)\n",
    "        next_pos = next_state[:2]        # agent(t+1)\n",
    "\n",
    "        to_target = F.normalize(target - pos, dim=0)\n",
    "        action_dir = F.normalize(action, dim=0)\n",
    "        direction_reward = torch.dot(action_dir, to_target)\n",
    "        direction_penalty = 1.0 - direction_reward\n",
    "\n",
    "        dist_before = torch.norm(pos - target)\n",
    "        dist_after = torch.norm(next_pos - target)  # sempre verso target(t)\n",
    "        progress = dist_before - dist_after\n",
    "\n",
    "        reward = - 5 * direction_penalty #+ progress\n",
    "\n",
    "        if torch.norm(next_state[:2] - state[2:4]) < tolerance:\n",
    "            reward += 100\n",
    "        \n",
    "        if rimbalzato:\n",
    "            reward -= 5\n",
    "\n",
    "        return reward - 1\n",
    "\n",
    "    def update(self, gamma=GAMMA, tau=TAU, device='cpu'):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = random.sample(self.buffer.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.FloatTensor(np.array(actions)).to(device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor_target(next_states)\n",
    "            target_Q = self.critic_target(next_states, next_actions)\n",
    "            y = rewards + gamma * target_Q * (1 - dones)\n",
    "\n",
    "        current_Q = self.critic(states, actions, training=True)\n",
    "        critic_loss = F.mse_loss(current_Q, y)\n",
    "\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        actor_loss = -self.critic(states, self.actor(states, training=True), training=True).mean()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "def save_checkpoint(agent, episode):\n",
    "    path = os.path.join(RUN_DIR, f\"checkpoint_ep{episode}.pth\")\n",
    "    torch.save({\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_state_dict': agent.critic.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def plot_and_save(rewards, successes):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards, label='Total Reward')\n",
    "    plt.plot(np.convolve(successes, np.ones(10)/10, mode='valid'), label='Success Rate (10)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.title('DDPG Training Progress')\n",
    "    plt.savefig(os.path.join(RUN_DIR, 'training_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "def save_trajectory_plot(trajectory, target_trajectory, episode, tag=\"trajectory\"):\n",
    "    trajectory = np.array(trajectory)\n",
    "    target_trajectory = np.array(target_trajectory)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(trajectory[:, 0], trajectory[:, 1], label=\"Agente\", color='blue')\n",
    "    plt.plot(target_trajectory[:, 0], target_trajectory[:, 1], label=\"Target\", color='red')\n",
    "    plt.scatter(*trajectory[0], color='green', label='Start agente', s=100)\n",
    "    plt.scatter(*target_trajectory[0], color='yellow', label='Start target', s=100)\n",
    "    plt.scatter(*target_trajectory[-1], color='red', label='End agente', s=100)\n",
    "    plt.scatter(target_trajectory[-5:, 0], target_trajectory[-5:, 1], color='orange', label='Ultimi target', s=10)\n",
    "    plt.scatter(trajectory[-5:, 0], trajectory[-5:, 1], color='purple', label='Ultimi agente', s=10)\n",
    "    plt.title(f\"{tag.capitalize()} - Episodio {episode}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "    plt.savefig(os.path.join(RUN_DIR, f\"{tag}_ep{episode}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def train_ddpg(env=None, num_episodes=6001):\n",
    "    if env is None:\n",
    "        env = TrackingEnv()\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    agent = DDPGAgent(state_dim, action_dim)\n",
    "    reward_history, success_history = [], []\n",
    "    counter = 0\n",
    "    tolerance = 0.02\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        agent.noise_std = max(agent.min_noise_std, agent.noise_std * agent.noise_decay)\n",
    "        trajectory, target_trajectory = [], []\n",
    "\n",
    "        while not done:\n",
    "            trajectory.append(state[:2].detach().numpy())\n",
    "            target_trajectory.append(state[2:4].detach().numpy())\n",
    "            action = agent.actor(state, training=True).detach().numpy()\n",
    "            noise = np.random.normal(0, agent.noise_std, size=action.shape)\n",
    "            noisy_action = action + noise\n",
    "            noisy_action = np.clip(noisy_action, env.action_space.low, env.action_space.high)\n",
    "            action_tensor = torch.tensor(noisy_action, dtype=torch.float32)\n",
    "\n",
    "            next_state, _, done, truncated, _, rimbalzato = env.step(noisy_action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            if torch.norm(next_state[:2] - next_state[2:4]) < tolerance or truncated:\n",
    "                done = True\n",
    "\n",
    "            reward = agent.reward_function(state, action_tensor, next_state, 0, tolerance, rimbalzato, 0)\n",
    "            transition = (state.numpy(), action_tensor.numpy(), reward, next_state.numpy(), float(done))\n",
    "            agent.buffer.push(transition)\n",
    "            if len(agent.buffer) > 1000:\n",
    "                agent.update()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if torch.norm(next_state[:2] - next_state[2:4]) < tolerance:\n",
    "            counter += 1\n",
    "            success_history.append(1)\n",
    "            if counter % 100 == 0:\n",
    "                save_trajectory_plot(trajectory, target_trajectory, episode, tag=\"success\")\n",
    "        else:\n",
    "            success_history.append(0)\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {total_reward:.2f}, Successes: {counter}\")\n",
    "        if episode % CHECKPOINT_INTERVAL == 0 and episode > 0:\n",
    "            save_checkpoint(agent, episode)\n",
    "        if episode % 500 == 0 and episode > 0:\n",
    "            save_trajectory_plot(trajectory, target_trajectory, episode)\n",
    "\n",
    "    np.save(os.path.join(RUN_DIR, 'rewards.npy'), reward_history)\n",
    "    np.save(os.path.join(RUN_DIR, 'successes.npy'), success_history)\n",
    "    plot_and_save(reward_history, success_history)\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent = train_ddpg()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
