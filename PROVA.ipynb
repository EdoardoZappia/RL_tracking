{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Attached_counter: 0, Reward: -381.40, Successes: 0\n",
      "Episode: 1, Attached_counter: 0, Reward: -337.98, Successes: 0\n",
      "Episode: 2, Attached_counter: 0, Reward: -411.70, Successes: 0\n",
      "Episode: 3, Attached_counter: 0, Reward: -343.01, Successes: 0\n",
      "Episode: 4, Attached_counter: 0, Reward: -419.37, Successes: 0\n",
      "Episode: 5, Attached_counter: 0, Reward: -392.33, Successes: 0\n",
      "Episode: 6, Attached_counter: 0, Reward: -413.70, Successes: 0\n",
      "Episode: 7, Attached_counter: 0, Reward: -399.89, Successes: 0\n",
      "Episode: 8, Attached_counter: 0, Reward: -408.85, Successes: 0\n",
      "Episode: 9, Attached_counter: 0, Reward: -441.37, Successes: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 299\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m     trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 260\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(env, num_episodes)\u001b[0m\n\u001b[1;32m    258\u001b[0m trajectory\u001b[38;5;241m.\u001b[39mappend(state[:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    259\u001b[0m target_trajectory\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m--> 260\u001b[0m action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m next_state, _, done, truncated, _, rimbalzato \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    262\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[2], line 133\u001b[0m, in \u001b[0;36mPPOAgent.get_action\u001b[0;34m(self, state, training)\u001b[0m\n\u001b[1;32m    131\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mu, sigma)\n\u001b[1;32m    132\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrsample()  \u001b[38;5;66;03m# Usa reparametrization trick per il backprop\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Somma log-prob per dimensione azione\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mdetach(), log_prob\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/fx/traceback.py:68\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from environment import TrackingEnv\n",
    "import datetime\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MAX_MEMORY = 4000\n",
    "NUM_NEURONS = 256\n",
    "LR_CRITIC = 0.0001\n",
    "LR_ACTOR = 0.0005\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "K_EPOCH = 10\n",
    "ENTROPY_COEFF = 0.01\n",
    "CHECKPOINT_INTERVAL = 500\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = f\"runs/PROVA{now}\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "def compute_advantages(rewards, values, dones, gamma=GAMMA, lam=LAMBDA):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = torch.cat([values, torch.tensor([0], dtype=values.dtype, device=values.device)])  # Append a 0 for the last state\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    return torch.tensor(advantages, dtype=torch.float)\n",
    "\n",
    "def compute_returns(rewards, dones, gamma=0.99):\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "        G = reward + gamma * G * (1 - done)  # Reset if episode ended\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        self.noise_std = 0.01\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        \n",
    "        # Output per la media (mu)\n",
    "        self.mu_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "        \n",
    "        # Output per la deviazione standard (log_sigma)\n",
    "        self.log_sigma_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "\n",
    "    def add_noise_to_target(self, state):\n",
    "        state = state.clone()\n",
    "        if state.dim() == 1:\n",
    "            #state[2:4] += torch.normal(0.0, self.noise_std, size=(2,), device=state.device)    # target only\n",
    "            state += torch.normal(0.0, self.noise_std, size=(4,), device=state.device)  # whole state\n",
    "        else:\n",
    "            #state[:, 2:4] += torch.normal(0.0, self.noise_std, size=state[:, 2:4].shape, device=state.device)  # target only\n",
    "            state += torch.normal(0.0, self.noise_std, size=state[:, :4].shape, device=state.device)    # whole state\n",
    "        return state\n",
    "\n",
    "\n",
    "    def forward(self, state, training=True):\n",
    "        \n",
    "        if training:\n",
    "            state = self.add_noise_to_target(state)\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Media delle azioni\n",
    "        mu = self.mu_layer(x)\n",
    "        mu = 5.0*torch.tanh(mu)    # azioni in [-5, 5]\n",
    "        \n",
    "        # Deviazione standard (softplus per garantire positività)\n",
    "        log_sigma = self.log_sigma_layer(x)\n",
    "        sigma = F.softplus(log_sigma) + 1e-5 # 1e-5 per evitare log(0)\n",
    "        #sigma = F.softplus(log_sigma) + exploration_term + 1e-5 # 1e-5 per evitare log(0)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc3 = nn.Linear(NUM_NEURONS, 1)\n",
    "        self.noise_std = 0.01\n",
    "\n",
    "    def add_noise_to_target(self, state):\n",
    "        state = state.clone()\n",
    "        if state.dim() == 1:\n",
    "            #state[2:4] += torch.normal(0.0, self.noise_std, size=(2,), device=state.device)    # target only\n",
    "            state += torch.normal(0.0, self.noise_std, size=(4,), device=state.device)  # whole state\n",
    "        else:\n",
    "            #state[:, 2:4] += torch.normal(0.0, self.noise_std, size=state[:, 2:4].shape, device=state.device)  # target only\n",
    "            state += torch.normal(0.0, self.noise_std, size=(4,), device=state.device)  # whole state\n",
    "        return state\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            x = self.add_noise_to_target(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.actor = PolicyNet(num_inputs, num_actions)\n",
    "        self.critic = ValueNet(num_inputs)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "        self.buffer = []\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        mu, sigma = self.actor.forward(state, training)\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "        action = dist.rsample()  # Usa reparametrization trick per il backprop\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)  # Somma log-prob per dimensione azione\n",
    "        return action.detach(), log_prob\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def reward_function(self, state, action, next_state, tolerance, rimbalzato, attached_counter):\n",
    "        #reward = - torch.norm(next_state[:2] - state[2:4]) -1\n",
    "        pos = state[:2]\n",
    "        target = state[2:4]              # target(t)\n",
    "        next_pos = next_state[:2]        # agent(t+1)\n",
    "\n",
    "        to_target = F.normalize(target - pos, dim=0)\n",
    "        action_dir = F.normalize(action, dim=0)\n",
    "        direction_reward = torch.dot(action_dir, to_target)\n",
    "        direction_penalty = 1.0 - direction_reward\n",
    "\n",
    "        reward = - direction_penalty #+ progress\n",
    "\n",
    "        if torch.norm(next_state[:2] - state[2:4]) < tolerance:\n",
    "            reward += 2 + 0.5 * attached_counter\n",
    "        \n",
    "        if rimbalzato:\n",
    "            reward -= 5\n",
    "\n",
    "        return reward - 1\n",
    "        \n",
    "    \n",
    "    def update(self):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        states, actions, rewards, dones, old_log_probs = zip(*self.buffer)\n",
    "        old_log_probs = torch.stack(old_log_probs)\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions).float()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        values = self.critic(states, training=True).squeeze()\n",
    "        advantages = compute_advantages(rewards, values.detach(), dones)\n",
    "        returns = compute_returns(rewards, dones)  # Compute Monte Carlo returns\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        \n",
    "        for _ in range(K_EPOCH):\n",
    "            means, stds = self.actor(states, training=True)\n",
    "            dist = torch.distributions.Normal(means, stds)\n",
    "            entropy = dist.entropy().sum(dim=-1).mean()  # media sull’intero batch\n",
    "            new_log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1-CLIP_EPS, 1+CLIP_EPS) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean() - ENTROPY_COEFF * entropy\n",
    "            values_pred =self.critic(states, training=True).squeeze()\n",
    "            value_loss = F.mse_loss(values_pred, returns)\n",
    "\n",
    "            total_loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            #policy_loss.backward()\n",
    "            #value_loss.backward()\n",
    "            total_loss.backward()\n",
    "            self.optimizer_actor.step()\n",
    "            self.optimizer_critic.step()\n",
    "        \n",
    "        self.buffer.clear()\n",
    "\n",
    "def save_checkpoint(agent, episode):\n",
    "    path = os.path.join(RUN_DIR, f\"checkpoint_ep{episode}.pth\")\n",
    "    torch.save({\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_state_dict': agent.critic.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def plot_and_save(rewards, successes):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards, label='Total Reward')\n",
    "    plt.plot(np.convolve(successes, np.ones(10)/10, mode='valid'), label='Success Rate (10)')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.title('PPO Training Progress')\n",
    "    plt.savefig(os.path.join(RUN_DIR, 'training_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "def save_trajectory_plot(trajectory, target_trajectory, episode, tag=\"trajectory\"):\n",
    "    trajectory = np.array(trajectory)\n",
    "    target_trajectory = np.array(target_trajectory)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(trajectory[:, 0], trajectory[:, 1], label=\"Agente\", color='blue')\n",
    "    plt.plot(target_trajectory[:, 0], target_trajectory[:, 1], label=\"Target\", color='red')\n",
    "    plt.scatter(*trajectory[0], color='green', label='Start agente', s=100)\n",
    "    plt.scatter(*target_trajectory[0], color='yellow', label='Start target', s=100)\n",
    "    plt.scatter(*target_trajectory[-1], color='red', label='End agente', s=100)\n",
    "    plt.scatter(target_trajectory[-5:, 0], target_trajectory[-5:, 1], color='orange', label='Ultimi target', s=10)\n",
    "    plt.scatter(trajectory[-5:, 0], trajectory[-5:, 1], color='purple', label='Ultimi agente', s=10)\n",
    "    plt.title(f\"{tag.capitalize()} - Episodio {episode}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "    plt.savefig(os.path.join(RUN_DIR, f\"{tag}_ep{episode}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def train_ppo(env=None, num_episodes=10001):\n",
    "    if env is None:\n",
    "        env = TrackingEnv()\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "    agent = PPOAgent(num_inputs, num_actions)\n",
    "    reward_history = []\n",
    "    success_history = []\n",
    "    counter = 0\n",
    "    tolerance = 0.02\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        trajectory = []\n",
    "        target_trajectory = []\n",
    "        step = 0\n",
    "        attached_counter = 0\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "            trajectory.append(state[:2].detach().numpy())\n",
    "            target_trajectory.append(state[2:4].detach().numpy())\n",
    "            action, log_prob = agent.get_action(state)\n",
    "            next_state, _, done, truncated, _, rimbalzato = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            if torch.norm(next_state[:2] - state[2:4]) > tolerance:\n",
    "                attached_counter = 0\n",
    "            else:\n",
    "                attached_counter += 1\n",
    "            done = truncated\n",
    "            reward = agent.reward_function(state, action, next_state, tolerance, rimbalzato, attached_counter)\n",
    "            agent.store_transition((state, action, reward, done, log_prob))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if attached_counter >= 30:\n",
    "            counter += 1\n",
    "            success_history.append(1)\n",
    "            if counter % 100 == 0:\n",
    "                save_trajectory_plot(trajectory, target_trajectory, episode, tag=\"success\")\n",
    "        else:\n",
    "            success_history.append(0)\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        if episode % 1 == 0:\n",
    "            print(f\"Episode: {episode}, Attached_counter: {attached_counter}, Reward: {total_reward:.2f}, Successes: {counter}\")\n",
    "            save_trajectory_plot(trajectory, target_trajectory, episode)\n",
    "            save_checkpoint(agent, episode)\n",
    "\n",
    "        if len(agent.buffer) >= MAX_MEMORY:\n",
    "            agent.update()\n",
    "\n",
    "    env.close()\n",
    "    np.save(os.path.join(RUN_DIR, 'rewards.npy'), reward_history)\n",
    "    np.save(os.path.join(RUN_DIR, 'successes.npy'), success_history)\n",
    "    plot_and_save(reward_history, success_history)\n",
    "    print(\"Training finished. Artifacts saved in:\", RUN_DIR)\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent = train_ppo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
