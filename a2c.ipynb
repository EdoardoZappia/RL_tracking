{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean action x: -0.2319922298192978\n",
      "Episode: 0, Step: 5226, Counter: 0, Avv: 2016 All: 3210, Total reward: -1.5767444372177124, final state: tensor([-0.6237,  2.0750,  0.3000,  0.3000])\n",
      "Episode 0, Total Reward: -1.5767444372177124\n",
      "mean action x: -0.313232421875\n",
      "Episode: 1, Step: 5068, Counter: 0, Avv: 1872 All: 3196, Total reward: -1.57728111743927, final state: tensor([-1.2523,  1.5636,  0.3000,  0.3000])\n",
      "mean action x: 0.05774411931633949\n",
      "Episode: 2, Step: 1889, Counter: 0, Avv: 504 All: 1385, Total reward: -1.5758062601089478, final state: tensor([-1.4504,  1.2677,  0.3000,  0.3000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 209\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m     trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_a2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 176\u001b[0m, in \u001b[0;36mtrain_a2c\u001b[0;34m(env, num_episodes)\u001b[0m\n\u001b[1;32m    168\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#if torch.norm(next_state[:2] - target[:2])<tolerance:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m#    reward = 100*reward # se si avvicina abbastanza ottiene un reward molto alto\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#    print(reward)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m#    done = True\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m#reward = torch.tensor(reward, dtype=torch.float32)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    178\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[1], line 126\u001b[0m, in \u001b[0;36mAgent.update\u001b[0;34m(self, state, action, log_prob, next_state, reward, done)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_critic\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_actor\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 126\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_actor\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment import TrackingEnv\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MAX_EPISODES = 1200\n",
    "NUM_NEURONS = 128\n",
    "LR_CRITIC = 0.0005\n",
    "LR_ACTOR = 0.0001\n",
    "GAMMA = 0.99\n",
    "#ENTROPY_COEF = 0.01\n",
    "MAX_STEP_EXPLORATION = 1e5\n",
    "EARLY_STOPPING_EPISODES = 30\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        \n",
    "        # Output per la media (mu)\n",
    "        self.mu_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "        \n",
    "        # Output per la deviazione standard (log_sigma)\n",
    "        self.log_sigma_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "\n",
    "    def forward(self, state, exploration_term):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Media delle azioni\n",
    "        mu = self.mu_layer(x)\n",
    "        \n",
    "        # Deviazione standard (softplus per garantire positività)\n",
    "        log_sigma = self.log_sigma_layer(x)\n",
    "        #sigma = F.softplus(log_sigma) + 1e-5 # 1e-5 per evitare log(0)\n",
    "        sigma = F.softplus(log_sigma) + exploration_term + 1e-5 # 1e-5 per evitare log(0)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc3 = nn.Linear(NUM_NEURONS, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.target = []    # il target verrà dato dall'ambiente\n",
    "        self.actor = PolicyNet(num_inputs, num_actions)\n",
    "        self.critic = ValueNet(num_inputs)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        self.counter_avvicinamento = 0\n",
    "        self.conuter_allontanamento = 0\n",
    "\n",
    "    def sample_action(self, state, exploration_term):\n",
    "        mu, sigma = self.actor.forward(state, exploration_term)\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "        action = dist.rsample()  # Usa reparametrization trick per il backprop\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)  # Somma log-prob per dimensione azione \n",
    "        return action, log_prob\n",
    "\n",
    "    def reward_function(self, state, next_state):\n",
    "        #state = torch.tensor(state, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        diff_t = state[:2]-state[2:]    # posizione target è nello stato\n",
    "        diff_t1 = next_state[:2]-next_state[2:]  # posizione target è nello stato\n",
    "\n",
    "        if torch.norm(diff_t) > torch.norm(diff_t1):\n",
    "            self.counter_avvicinamento += 1\n",
    "        else:\n",
    "            self.counter_allontanamento += 1\n",
    "\n",
    "        return torch.norm(diff_t) - torch.norm(diff_t1)\n",
    "        #reward = 0\n",
    "        #if torch.norm(diff_t) >= torch.norm(diff_t1):\n",
    "        #    reward = 1\n",
    "        #elif torch.norm(diff_t1) < 0.1:\n",
    "        #    reward = 100\n",
    "        #elif torch.norm(diff_t) < torch.norm(diff_t1):\n",
    "        #    reward = -1\n",
    "        #return reward\n",
    "\n",
    "    def get_exploration_term(self, current_step, max_steps):\n",
    "        return max(0.0, 0.2 * (1 - current_step / max_steps))  # lineare decrescente\n",
    "\n",
    "    def update(self, state, action, log_prob, next_state, reward, done):\n",
    "        #state = torch.tensor(state, dtype=torch.float32)\n",
    "        #action = torch.tensor(action, dtype=torch.int64)\n",
    "        #next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        #reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "\n",
    "        value = self.critic(state).squeeze()\n",
    "        target_value = reward + self.critic(next_state).squeeze() * GAMMA * (1 - done)\n",
    "        advantage = target_value - value\n",
    "\n",
    "        critic_loss = nn.MSELoss()(value, target_value.detach())\n",
    "        \n",
    "        actor_loss = -log_prob * advantage.detach()\n",
    "        \n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "\n",
    "def train_a2c(env=None, num_episodes=MAX_EPISODES):\n",
    "    #target = torch.tensor(target, dtype=torch.float32)\n",
    "    if env is None:\n",
    "        env = TrackingEnv()\n",
    "    inputs_dim = env.observation_space.shape[0]\n",
    "    actions_dim = env.action_space.shape[0]\n",
    "    #print(f\"Num inputs: {inputs_dim}, Num actions: {actions_dim}\")\n",
    "\n",
    "    agent = Agent(inputs_dim, actions_dim)#, target)\n",
    "    reward_history = []\n",
    "    total_step = 0\n",
    "    counter = 0\n",
    "    tolerance = 0.05\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        #print(f\"Episode: {episode}\")\n",
    "        agent.counter_avvicinamento = 0\n",
    "        agent.counter_allontanamento = 0\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        #mean_reward = 0\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        step = 0\n",
    "        actions = []\n",
    "        \n",
    "        while not done:\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            exploration_term = agent.get_exploration_term(total_step, MAX_STEP_EXPLORATION)\n",
    "            action, log_prob = agent.sample_action(state, exploration_term)\n",
    "            actions.append(action)\n",
    "            next_state, _, done, _, _ = env.step(action)\n",
    "            #print(f\"state: {state}, next_state: {next_state}\")\n",
    "            reward = agent.reward_function(state, next_state)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            #print(f\"state: {state}, next_state: {next_state}, reward: {reward}\")\n",
    "            if torch.norm(next_state[:2] - next_state[2:])>2 or torch.norm(next_state[:2] - next_state[2:])<tolerance:\n",
    "                done = True\n",
    "            #if torch.norm(next_state[:2] - target[:2])<tolerance:\n",
    "            #    reward = 100*reward # se si avvicina abbastanza ottiene un reward molto alto\n",
    "            #    print(reward)\n",
    "            #    done = True\n",
    "            #if torch.norm(next_state[:2] - target[:2])>5:\n",
    "            #    done = True\n",
    "            #reward = torch.tensor(reward, dtype=torch.float32)\n",
    "            agent.update(state, action, log_prob, next_state, reward, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            #print(f\"Episode: {episode}, Step: {step}, reward: {reward}\")\n",
    "\n",
    "        #mean_reward = total_reward/step\n",
    "        print(f\"mean action x: {action[0].mean()}, mean action y: {action[1].mean()}\")\n",
    "\n",
    "        print(f\"Episode: {episode}, Step: {step}, Counter: {counter}, Avv: {agent.counter_avvicinamento} All: {agent.counter_allontanamento}, Total reward: {total_reward}, final state: {state}\")\n",
    "        if torch.norm(next_state[:2] - next_state[2:])<tolerance:\n",
    "            counter += 1\n",
    "        if counter % 100 == 0 and counter != 0:\n",
    "            #counter = 0\n",
    "            tolerance = round(max(0.1, tolerance-0.1),2)\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        if len(reward_history)>EARLY_STOPPING_EPISODES and reward_history[EARLY_STOPPING_EPISODES:] > 1:\n",
    "            break\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    plt.plot(reward_history)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Progress Translation Agent')\n",
    "    plt.show()\n",
    "\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent = train_a2c()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
