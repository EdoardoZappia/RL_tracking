{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Step: 5226, Counter: 0, Avv: 2016 All: 3210, Total reward: -1.5767444372177124, final state: tensor([-0.6237,  2.0750,  0.3000,  0.3000])\n",
      "Episode 0, Total Reward: -1.5767444372177124\n",
      "Episode: 1, Step: 5068, Counter: 0, Avv: 1872 All: 3196, Total reward: -1.57728111743927, final state: tensor([-1.2523,  1.5636,  0.3000,  0.3000])\n",
      "Episode: 2, Step: 1889, Counter: 0, Avv: 504 All: 1385, Total reward: -1.5758062601089478, final state: tensor([-1.4504,  1.2677,  0.3000,  0.3000])\n",
      "Episode: 3, Step: 1532, Counter: 0, Avv: 230 All: 1302, Total reward: -1.5761796236038208, final state: tensor([-1.5009,  1.1709,  0.3000,  0.3000])\n",
      "Episode: 4, Step: 1975, Counter: 0, Avv: 458 All: 1517, Total reward: -1.5764631032943726, final state: tensor([-1.3396,  1.4466,  0.3000,  0.3000])\n",
      "Episode: 5, Step: 2048, Counter: 0, Avv: 443 All: 1605, Total reward: -1.5774205923080444, final state: tensor([-1.4897,  1.1966,  0.3000,  0.3000])\n",
      "Episode: 6, Step: 3408, Counter: 0, Avv: 1016 All: 2392, Total reward: -1.5764824151992798, final state: tensor([-1.1708,  1.6563,  0.3000,  0.3000])\n",
      "Episode: 7, Step: 1818, Counter: 0, Avv: 352 All: 1466, Total reward: -1.5769680738449097, final state: tensor([-1.4264,  1.3121,  0.3000,  0.3000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 220\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m     trained_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_a2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 180\u001b[0m, in \u001b[0;36mtrain_a2c\u001b[0;34m(env, num_episodes)\u001b[0m\n\u001b[1;32m    172\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m#if torch.norm(next_state[:2] - target[:2])<tolerance:\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m#    reward = 100*reward # se si avvicina abbastanza ottiene un reward molto alto\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m#    print(reward)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#    done = True\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m#reward = torch.tensor(reward, dtype=torch.float32)\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    182\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m, in \u001b[0;36mAgent.update\u001b[0;34m(self, state, action, log_prob, next_state, reward, done)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_actor\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    126\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment import TrackingEnv\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MAX_EPISODES = 1200\n",
    "NUM_NEURONS = 128\n",
    "LR_CRITIC = 0.0005\n",
    "LR_ACTOR = 0.0001\n",
    "GAMMA = 0.99\n",
    "#ENTROPY_COEF = 0.01\n",
    "MAX_STEP_EXPLORATION = 1e5\n",
    "EARLY_STOPPING_EPISODES = 30\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        \n",
    "        # Output per la media (mu)\n",
    "        self.mu_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "        \n",
    "        # Output per la deviazione standard (log_sigma)\n",
    "        self.log_sigma_layer = nn.Linear(NUM_NEURONS, action_dim)\n",
    "\n",
    "    def forward(self, state, exploration_term):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Media delle azioni\n",
    "        mu = self.mu_layer(x)\n",
    "        \n",
    "        # Deviazione standard (softplus per garantire positività)\n",
    "        log_sigma = self.log_sigma_layer(x)\n",
    "        #sigma = F.softplus(log_sigma) + 1e-5 # 1e-5 per evitare log(0)\n",
    "        sigma = F.softplus(log_sigma) + exploration_term + 1e-5 # 1e-5 per evitare log(0)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc3 = nn.Linear(NUM_NEURONS, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.target = []    # il target verrà dato dall'ambiente\n",
    "        self.actor = PolicyNet(num_inputs, num_actions)\n",
    "        self.critic = ValueNet(num_inputs)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=LR_ACTOR)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        self.counter_avvicinamento = 0\n",
    "        self.conuter_allontanamento = 0\n",
    "\n",
    "    def sample_action(self, state, exploration_term):\n",
    "        mu, sigma = self.actor.forward(state, exploration_term)\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "        action = dist.rsample()  # Usa reparametrization trick per il backprop\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)  # Somma log-prob per dimensione azione \n",
    "        return action, log_prob\n",
    "\n",
    "    def reward_function(self, state, next_state):\n",
    "        #state = torch.tensor(state, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        diff_t = state[:2]-state[2:]    # posizione target è nello stato\n",
    "        diff_t1 = next_state[:2]-next_state[2:]  # posizione target è nello stato\n",
    "\n",
    "        if torch.norm(diff_t) > torch.norm(diff_t1):\n",
    "            self.counter_avvicinamento += 1\n",
    "        else:\n",
    "            self.counter_allontanamento += 1\n",
    "\n",
    "        return torch.norm(diff_t) - torch.norm(diff_t1)\n",
    "        #reward = 0\n",
    "        #if torch.norm(diff_t) >= torch.norm(diff_t1):\n",
    "        #    reward = 1\n",
    "        #elif torch.norm(diff_t1) < 0.1:\n",
    "        #    reward = 100\n",
    "        #elif torch.norm(diff_t) < torch.norm(diff_t1):\n",
    "        #    reward = -1\n",
    "        #return reward\n",
    "\n",
    "    def get_exploration_term(self, current_step, max_steps):\n",
    "        return max(0.0, 0.2 * (1 - current_step / max_steps))  # lineare decrescente\n",
    "\n",
    "    def update(self, state, action, log_prob, next_state, reward, done):\n",
    "        #state = torch.tensor(state, dtype=torch.float32)\n",
    "        #action = torch.tensor(action, dtype=torch.int64)\n",
    "        #next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        #reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "\n",
    "        value = self.critic(state).squeeze()\n",
    "        target_value = reward + self.critic(next_state).squeeze() * GAMMA * (1 - done)\n",
    "        advantage = target_value - value\n",
    "\n",
    "        critic_loss = nn.MSELoss()(value, target_value.detach())\n",
    "        \n",
    "        actor_loss = -log_prob * advantage.detach()\n",
    "        \n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "\n",
    "def train_a2c(env=None, num_episodes=MAX_EPISODES):\n",
    "    #target = torch.tensor(target, dtype=torch.float32)\n",
    "    if env is None:\n",
    "        env = TrackingEnv()\n",
    "    inputs_dim = env.observation_space.shape[0]\n",
    "    actions_dim = env.action_space.shape[0]\n",
    "    #print(f\"Num inputs: {inputs_dim}, Num actions: {actions_dim}\")\n",
    "\n",
    "    agent = Agent(inputs_dim, actions_dim)#, target)\n",
    "    reward_history = []\n",
    "    total_step = 0\n",
    "    counter = 0\n",
    "    tolerance = 0.05\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        #print(f\"Episode: {episode}\")\n",
    "        agent.counter_avvicinamento = 0\n",
    "        agent.counter_allontanamento = 0\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        #mean_reward = 0\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        step = 0\n",
    "        #actions_x = []\n",
    "        #actions_y = []\n",
    "        #max_actions_x = []\n",
    "        #max_actions_y = []\n",
    "        \n",
    "        while not done:\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            exploration_term = agent.get_exploration_term(total_step, MAX_STEP_EXPLORATION)\n",
    "            action, log_prob = agent.sample_action(state, exploration_term)\n",
    "            actions_x.append(action[0])\n",
    "            actions_y.append(action[1])\n",
    "            next_state, _, done, _, _ = env.step(action)\n",
    "            #print(f\"state: {state}, next_state: {next_state}\")\n",
    "            reward = agent.reward_function(state, next_state)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            #print(f\"state: {state}, next_state: {next_state}, reward: {reward}\")\n",
    "            if torch.norm(next_state[:2] - next_state[2:])>2 or torch.norm(next_state[:2] - next_state[2:])<tolerance:\n",
    "                done = True\n",
    "            #if torch.norm(next_state[:2] - target[:2])<tolerance:\n",
    "            #    reward = 100*reward # se si avvicina abbastanza ottiene un reward molto alto\n",
    "            #    print(reward)\n",
    "            #    done = True\n",
    "            #if torch.norm(next_state[:2] - target[:2])>5:\n",
    "            #    done = True\n",
    "            #reward = torch.tensor(reward, dtype=torch.float32)\n",
    "            agent.update(state, action, log_prob, next_state, reward, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            #print(f\"Episode: {episode}, Step: {step}, reward: {reward}\")\n",
    "\n",
    "        #if episode % 10 == 0:\n",
    "        #    max_actions_x.append(max([action.max().item() for action in actions_x]))\n",
    "        #    max_actions_y.append(max([action.max().item() for action in actions_y]))\n",
    "\n",
    "        #    mean_action_x = torch.mean(torch.tensor(actions_x)).item()\n",
    "        #    mean_action_y = torch.mean(torch.tensor(actions_y)).item()\n",
    "        #    print(f\"mean action x: {mean_action_x}, mean action y: {mean_action_y}\")\n",
    "        #    print(f\"max action x: {max_actions_x}, max action y: {max_actions_y}\")\n",
    "        #print(f\"mean action x: {torch.mean(actions_x).item()}, mean action y: {torch.mean(actions_y).item()}\")\n",
    "\n",
    "        print(f\"Episode: {episode}, Step: {step}, Counter: {counter}, Avv: {agent.counter_avvicinamento} All: {agent.counter_allontanamento}, Total reward: {total_reward}, final state: {state}\")\n",
    "        if torch.norm(next_state[:2] - next_state[2:])<tolerance:\n",
    "            counter += 1\n",
    "        #if counter % 100 == 0 and counter != 0:\n",
    "            #counter = 0\n",
    "        #    tolerance = round(max(0.1, tolerance-0.1),2)\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        #if len(reward_history)>EARLY_STOPPING_EPISODES and reward_history[EARLY_STOPPING_EPISODES:] > 1:\n",
    "        #    break\n",
    "        \n",
    "        #if episode % 10 == 0:\n",
    "        #    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    plt.plot(reward_history)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Training Progress Translation Agent')\n",
    "    plt.show()\n",
    "\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent = train_a2c()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
